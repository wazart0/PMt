{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from atlassian import Jira\n",
    "from bidict import bidict\n",
    "\n",
    "# jira_info = {\n",
    "#     'url': 'https://tangramcare.atlassian.net/',\n",
    "#     'username': 'xxx@ownedoutcomes.com',\n",
    "#     'api_token': 'xxxxxxxxxxxxxxxx',\n",
    "# }\n",
    "\n",
    "jira_info = {\n",
    "    'url': 'https://tangramcare.atlassian.net/',\n",
    "    'username': 'awaz@ownedoutcomes.com',\n",
    "    'api_token': '6Pu9J7zSN4wBqwqgSREsAE08',\n",
    "}\n",
    "\n",
    "# jira_info = {\n",
    "#     'url': 'https://awaz.atlassian.net/',\n",
    "#     'username': 'wazartur@gmail.com',\n",
    "#     'api_token': 'typYw7sWGOPzOLvtq5Z91BDC',\n",
    "# }\n",
    "\n",
    "\n",
    "# jira standard fields config\n",
    "jira_standard_fields = bidict({\n",
    "    'Summary': 'summary',\n",
    "    'Epic': 'customfield_10008',\n",
    "    'Description': 'description',\n",
    "    'Project': 'project',\n",
    "    'Type': 'issuetype',\n",
    "    'Priority': 'priority',\n",
    "    'Labels': 'labels',\n",
    "    'Status': 'status',\n",
    "    'Creator': 'creator',\n",
    "    'Assignee': 'assignee',\n",
    "    'Parent': 'parent',\n",
    "})\n",
    "\n",
    "jira = Jira(\n",
    "            url=jira_info['url'],\n",
    "            username=jira_info['username'],\n",
    "            password=jira_info['api_token'],\n",
    "            cloud=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "100\n0\n"
     ]
    }
   ],
   "source": [
    "issues = jira.jql('project=tt', fields=list(o2_pp_fields.inv.keys()), limit=1000)\n",
    "print(str(issues['maxResults']))\n",
    "print(str(len(issues['issues'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Updating issue \"PP-329\" with \"{'description': 'check', 'customfield_11701': '2021-03-26', 'customfield_11702': '2021-03-27'}\"\n"
     ]
    }
   ],
   "source": [
    "jira.issue_update('PP-329', {\n",
    "            'description': 'check',\n",
    "            'customfield_11701': '2021-03-26',\n",
    "            'customfield_11702': '2021-03-27'\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "\n",
    "workbook = Workbook()\n",
    "sheet = workbook.active\n",
    "\n",
    "sheet[\"A1\"].value = \"Design document:\"\n",
    "sheet[\"C1\"].value = \"Link\"\n",
    "sheet[\"C1\"].hyperlink = \"https://docs.google.com/document/d/1B5RlsVSZdiuKTVAqhrn7xClewqhzrxhliUZd_yIHUBg/edit?usp=sharing\"\n",
    "sheet[\"C1\"].style = \"Hyperlink\"\n",
    "\n",
    "sheet.merge_cells('A3:A4')\n",
    "sheet['A3'].value = 'WBS'\n",
    "\n",
    "sheet.merge_cells('B3:B4')\n",
    "sheet['B3'].value = 'Name'\n",
    "\n",
    "sheet.merge_cells('C3:C4')\n",
    "sheet['C3'].value = 'Description'\n",
    "\n",
    "sheet.merge_cells('D3:D4')\n",
    "sheet['D3'].value = 'Depends on'\n",
    "\n",
    "sheet.merge_cells('E3:G3')\n",
    "sheet['E3'].value = 'Estimated development time [h]'\n",
    "sheet['E4'].value = 'Designer'\n",
    "sheet['F4'].value = 'Reviewer'\n",
    "sheet['G4'].value = 'PM'\n",
    "\n",
    "sheet.merge_cells('H3:H4')\n",
    "sheet['H3'].value = 'Assigned developer'\n",
    "\n",
    "sheet.merge_cells('I3:J3')\n",
    "sheet['I3'].value = 'Estimated testing time [h]'\n",
    "sheet['I4'].value = 'Tester'\n",
    "sheet['J4'].value = 'PM'\n",
    "\n",
    "sheet.merge_cells('K3:K4')\n",
    "sheet['K3'].value = 'Assigned tester'\n",
    "\n",
    "sheet.merge_cells('L3:N3')\n",
    "sheet['L3'].value = 'Actual development time'\n",
    "sheet['L4'].value = 'Start time'\n",
    "sheet['M4'].value = 'Finish time'\n",
    "sheet['N4'].value = 'Duration [h]'\n",
    "\n",
    "sheet.merge_cells('O3:P3')\n",
    "sheet['O3'].value = 'Commit statistics'\n",
    "sheet['O4'].value = 'Insertions'\n",
    "sheet['P4'].value = 'Deletions'\n",
    "\n",
    "sheet.merge_cells('Q3:Q4')\n",
    "sheet['Q3'].value = 'Estimated code documentation time [h]'\n",
    "\n",
    "sheet.merge_cells('R3:R4')\n",
    "sheet['R3'].value = 'Jira ID'\n",
    "\n",
    "sheet.merge_cells('S3:S4')\n",
    "sheet['S3'].value = 'Commit ID'\n",
    "\n",
    "sheet.merge_cells('T3:T4')\n",
    "sheet['T3'].value = 'Comment'\n",
    "\n",
    "\n",
    "workbook.save(filename=\"hello_world.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_to_date_converter(date):\n",
    "    from datetime import datetime\n",
    "    from dateutil import parser\n",
    "    return parser.parse(date).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def get_schema_from_sheet(sheet): # TODO schema validation\n",
    "    schema = {}\n",
    "    schema['offset_row'] = 1 # offset of WBS: TODO search for WBS\n",
    "    schema['offset_col'] = 0 # offset of WBS: TODO search for WBS\n",
    "    # schema['header_1'] = [i.value for i in sheet['1']] # TODO apply offsets\n",
    "    # schema['header_2'] = [i.value for i in sheet['2']] # TODO apply offsets\n",
    "    # schema['map_to_jira'] = { # TODO something with history (start, finish, duration columns)\n",
    "    #     'Name': 'summary',\n",
    "    #     'Description': 'description',\n",
    "    #     'Jira ID': 'key',\n",
    "    # }\n",
    "\n",
    "\n",
    "    # schema['columns'] = { # TODO make it dynamic\n",
    "    #     'WBS': 'A',\n",
    "    #     'Name': 'B',\n",
    "    #     'Description': 'C',\n",
    "    #     'Depends on': 'D',\n",
    "    #     'Estimate PM': 'G',\n",
    "    #     'Jira ID': 'R',\n",
    "    #     'Start time': 'L',\n",
    "    #     'Finish time': 'M',\n",
    "    #     'Duration [h]': 'N'\n",
    "    # }\n",
    "    # schema['data_offset_row'] = 3\n",
    "    \n",
    "    schema['columns'] = { # TODO make it dynamic\n",
    "        'WBS': 'A',\n",
    "        'Name': 'B',\n",
    "        'Description': 'C',\n",
    "        'Depends on': 'D',\n",
    "        'Estimate PM': 'G',\n",
    "        'Jira ID': 'P',\n",
    "        'Start time': 'J',\n",
    "        'Finish time': 'K',\n",
    "        'Duration [h]': 'L'\n",
    "    }\n",
    "    schema['data_offset_row'] = 3\n",
    "\n",
    "    return schema\n",
    "\n",
    "\n",
    "def get_default_schema_from_template_xlsx(input_template_file = 'template.xlsx', output_schema_file = None):\n",
    "    from openpyxl import load_workbook\n",
    "    import json\n",
    "    workbook = load_workbook(filename=input_template_file)\n",
    "    sheet = workbook['Tasks']\n",
    "\n",
    "    schema = get_schema_from_sheet(sheet)\n",
    "\n",
    "    if output_schema_file == None:\n",
    "        return schema\n",
    "    else:\n",
    "        json.dump(schema, open(output_schema_file, 'w'))\n",
    "\n",
    "\n",
    "def get_issues_from_jira_and_create_xlsx_document_from_template(jira, output_file, project, label, input_template_file = 'template.xlsx'):\n",
    "    from openpyxl import load_workbook\n",
    "    import json\n",
    "    workbook = load_workbook(filename=input_template_file)\n",
    "    sheet = workbook['Tasks']\n",
    "\n",
    "    schema = get_schema_from_sheet(sheet)\n",
    "\n",
    "    issues = jira.jql('project=' + str(project), fields=list(jira_standard_fields.inv.keys()), limit=1000)\n",
    "    # issues = jira.jql('project=' + str(project) + ' and labels=' + str(label), fields=list(jira_standard_fields.inv.keys()), limit=1000)\n",
    "\n",
    "    # TODO\n",
    "\n",
    "\n",
    "def xlsx_to_jira(jira, project, label, input_filename):\n",
    "    from openpyxl import load_workbook\n",
    "    \n",
    "    workbook = load_workbook(filename=input_template_file)\n",
    "    sheet = workbook['Tasks']\n",
    "\n",
    "    schema = get_schema_from_sheet(sheet)\n",
    "    \n",
    "\n",
    "def wbs_regex_check(string, separator = '.'):\n",
    "    import re\n",
    "    return True if re.match('\\A([0-9]+\\\\' + separator + '?)+\\Z', string) else False\n",
    "\n",
    "def wbs_regex_task(string):\n",
    "    import re\n",
    "    return True if re.match('\\A[0-9]+\\Z', string) else False\n",
    "\n",
    "def check_wbs_level(wbs, separator = '.'):\n",
    "    return len(wbs.split(separator))\n",
    "\n",
    "\n",
    "def get_wbs_ancestor(wbs, separator = '.'):\n",
    "    if len(wbs.split(wbs)) == 1:\n",
    "        return wbs.split(wbs)[0]\n",
    "    else:\n",
    "        return '.'.join(wbs.split('.')[0:-1])\n",
    "\n",
    "\n",
    "def get_wbs_lowest_level_task_number(wbs, separator = '.'):\n",
    "    return wbs.split(separator)[-1]\n",
    "\n",
    "\n",
    "def expand_wbs_range(start, finish, separator = '.'):\n",
    "    if check_wbs_level(start, separator) != check_wbs_level(finish, separator):\n",
    "        print('ERROR: WBS range level differs: ', start, ' - ', finish)\n",
    "        return []\n",
    "    if get_wbs_ancestor(start, separator) != get_wbs_ancestor(finish, separator):\n",
    "        print('ERROR: WBS range ancestor differs: ', start, ' - ', finish)\n",
    "        return []\n",
    "    lowest_level_task = list(range(int(get_wbs_lowest_level_task_number(start, separator)), 1 + int(get_wbs_lowest_level_task_number(finish, separator))))\n",
    "    wbs_tasks = []\n",
    "    for i in lowest_level_task:\n",
    "        if get_wbs_ancestor(start, separator) == '':\n",
    "            wbs_tasks.append(str(i))\n",
    "        else:\n",
    "            wbs_tasks.append(get_wbs_ancestor(start, separator) + '.' + str(i))\n",
    "    return wbs_tasks\n",
    "    \n",
    "def parse_dependencies(dependency_str, wbs_separator = '.'):\n",
    "    wbs_tasks = []\n",
    "    for i in str(dependency_str).split(','):\n",
    "        if wbs_regex_check(i.strip(), wbs_separator):\n",
    "            wbs_tasks.append(i)\n",
    "        else:\n",
    "            range = i.split('-')\n",
    "            if len(range) != 2:\n",
    "                print(\"ERROR: It is not a range: \", range)\n",
    "                continue\n",
    "            if wbs_regex_check(range[0].strip(), wbs_separator) == False or wbs_regex_check(range[1].strip(), wbs_separator) == False:\n",
    "                print(\"ERROR: It is not a range: \", range)\n",
    "                continue\n",
    "            wbs_tasks = wbs_tasks + expand_wbs_range(range[0].strip(), range[1].strip(), wbs_separator)\n",
    "    return wbs_tasks\n",
    "\n",
    "\n",
    "\n",
    "def get_jira_key_for_wbs(schema, wbs):\n",
    "    i = schema['data_offset_row']\n",
    "    while sheet[schema['columns']['WBS'] + str(i)].value is not None:\n",
    "        if wbs == str(sheet[schema['columns']['WBS'] + str(i)].value):\n",
    "            return str(sheet[schema['columns']['Jira ID'] + str(i)].value)\n",
    "        i = i + 1 \n",
    "    return None\n",
    "\n",
    "\n",
    "def create_or_update_issue(jira, schema, index, project, type, label, parent = None):\n",
    "    if sheet[schema['columns']['Jira ID'] + str(index)].value is None: # create issue\n",
    "        issue = jira.issue_create({\n",
    "            'summary': sheet[schema['columns']['Name'] + str(index)].value,\n",
    "            'description': sheet[schema['columns']['Description'] + str(index)].value,\n",
    "            'project': {'key': project},\n",
    "            'issuetype': {'name': type},\n",
    "            'parent': {'key': parent},\n",
    "            'customfield_11701': sheet[schema['columns']['Start time'] + str(index)].value,\n",
    "            'customfield_11702': sheet[schema['columns']['Finish time'] + str(index)].value,\n",
    "            # 'customfield_10004': str(float(sheet[schema['columns']['Estimate PM'] + str(index)].value)/4),\n",
    "            # 'Priority': 'priority',\n",
    "            # 'Status': 'status',\n",
    "            # 'Creator': 'creator',\n",
    "            # 'Assignee': 'assignee',\n",
    "        })\n",
    "        sheet[schema['columns']['Jira ID'] + str(index)].value = issue['key']\n",
    "        sheet[schema['columns']['Jira ID'] + str(index)].hyperlink = jira.url + '/browse/' + issue['key']\n",
    "        sheet[schema['columns']['Jira ID'] + str(index)].font = Font(underline='single', color='0000FF')\n",
    "        print(issue)\n",
    "    else:\n",
    "        task_json = {\n",
    "            'summary': sheet[schema['columns']['Name'] + str(index)].value,\n",
    "            'description': sheet[schema['columns']['Description'] + str(index)].value\n",
    "        }\n",
    "\n",
    "        if sheet[schema['columns']['Start time'] + str(index)].value is not None:\n",
    "            task_json['customfield_11701'] = datetime_to_date_converter(sheet[schema['columns']['Start time'] + str(index)].value)\n",
    "        if sheet[schema['columns']['Finish time'] + str(index)].value is not None:\n",
    "            task_json['customfield_11702'] = datetime_to_date_converter(sheet[schema['columns']['Finish time'] + str(index)].value)\n",
    "\n",
    "        if sheet[schema['columns']['Duration [h]'] + str(index)].value is not None:\n",
    "            task_json['customfield_10004'] = float(sheet[schema['columns']['Duration [h]'] + str(index)].value)/4\n",
    "        else:\n",
    "            print(sheet[schema['columns']['Estimate PM'] + str(index)].value)\n",
    "            if sheet[schema['columns']['Estimate PM'] + str(index)].value is not None:\n",
    "                task_json['customfield_10004'] = float(sheet[schema['columns']['Estimate PM'] + str(index)].value)/4\n",
    "                \n",
    "        jira.issue_update(sheet[schema['columns']['Jira ID'] + str(index)].value, task_json)\n",
    "\n",
    "    if sheet[schema['columns']['Depends on'] + str(index)].value is not None:\n",
    "        if sheet[schema['columns']['Depends on'] + str(index)].value != '':\n",
    "            for dependent_task_wbs in parse_dependencies(sheet[schema['columns']['Depends on'] + str(index)].value):\n",
    "                fields = {\n",
    "                    \"issuelinks\": [\n",
    "                        {\n",
    "                            \"add\": {\n",
    "                                \"type\": {\n",
    "                                    \"name\": \"Gantt End to Start\",\n",
    "                                    \"inward\": \"has to be done after\",\n",
    "                                    \"outward\": \"has to be done before\"\n",
    "                                },\n",
    "                                \"inwardIssue\": {\n",
    "                                    \"key\": get_jira_key_for_wbs(schema, dependent_task_wbs.strip())\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "                jira.edit_issue(issue_id_or_key=sheet[schema['columns']['Jira ID'] + str(index)].value, fields=fields, notify_users=False)\n",
    "    jira.edit_issue(issue_id_or_key=sheet[schema['columns']['Jira ID'] + str(index)].value, fields={\"labels\": [{\"add\": label}]}, notify_users=False)\n",
    "\n",
    "\n",
    "def create_or_update_issues(jira, sheet, schema, project, label):\n",
    "    i = schema['data_offset_row']\n",
    "    while sheet[schema['columns']['WBS'] + str(i)].value is not None:\n",
    "        wbs = str(sheet[schema['columns']['WBS'] + str(i)].value)\n",
    "        if not wbs_regex_check(wbs):\n",
    "            raise Exception('Problem in WBS number structure: ' + str(wbs))\n",
    "\n",
    "        if wbs_regex_task(wbs):\n",
    "            create_or_update_issue(jira=jira, schema=schema, index=i, project=project, type='Task', label=label)\n",
    "        else:\n",
    "            import re\n",
    "            parent = get_jira_key_for_wbs(schema=schema, wbs=re.match('\\A[0-9]+', wbs)[0])\n",
    "            if parent is None:\n",
    "                raise Exception('Task doesn\\'t have a parent: ' + wbs)\n",
    "            create_or_update_issue(jira=jira, schema=schema, index=i, project=project, type='Sub-task', label=label, parent=parent)\n",
    "        # if i > schema['data_offset_row'] + 5: # for tests purposes\n",
    "        #     return\n",
    "        i = i + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import Font\n",
    "\n",
    "# filename = 'grouper.xlsx'\n",
    "# label = 'P2_pipelines'\n",
    "\n",
    "filename = 'p2v3.xlsx'\n",
    "label = 'P2_v3'\n",
    "\n",
    "# filename = 'auth.xlsx'\n",
    "# label = 'Authorization'\n",
    "\n",
    "# filename = 'tests.xlsx'\n",
    "# label = 'Tests'\n",
    "\n",
    "workbook = load_workbook(filename=filename)\n",
    "sheet = workbook['Tasks']\n",
    "\n",
    "schema = get_schema_from_sheet(sheet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "AnalyticalService', 'description': None, 'customfield_11701': '2021-03-18', 'customfield_11702': '2021-03-19', 'customfield_10004': 2.0}\"\n",
      "Updating issue \"PP-400\" with \"{'summary': 'Implement find_dataset_time_range', 'description': 'use mock DatasetService for tests', 'customfield_11701': '2021-03-19', 'customfield_11702': '2021-03-22', 'customfield_10004': 1.5}\"\n",
      "Updating issue \"PP-401\" with \"{'summary': 'Implement find_benchmark_datasets method', 'description': 'use mock DatasetService for tests', 'customfield_11701': '2021-03-22', 'customfield_11702': '2021-03-24', 'customfield_10004': 5.0}\"\n",
      "Updating issue \"PP-402\" with \"{'summary': 'Implement calculate_benchmark method in PandasAnalyticalService', 'description': 'for now do not implement any special handling of AC_Entity / PAC_Entity nodes, do not handle packed nodes', 'customfield_11701': '2021-03-19', 'customfield_10004': 12.0}\"\n",
      "48\n",
      "Updating issue \"PP-403\" with \"{'summary': 'Implement get_service_line in PandasAnalyticalService', 'description': None, 'customfield_11701': '2021-03-23', 'customfield_11702': '2021-03-24', 'customfield_10004': 1.5}\"\n",
      "Updating issue \"PP-404\" with \"{'summary': 'Implement get_clinical_episode in PandasAnalyticalService', 'description': None, 'customfield_11701': '2021-03-23', 'customfield_11702': '2021-03-24', 'customfield_10004': 1.0}\"\n",
      "Updating issue \"PP-405\" with \"{'summary': 'Implement get_parent_node method', 'description': None, 'customfield_11701': '2021-03-24', 'customfield_11702': '2021-03-25', 'customfield_10004': 1.0}\"\n",
      "Updating issue \"PP-406\" with \"{'summary': 'Handle parent nodes in calculate_benchmark method', 'description': 'see design', 'customfield_10004': 4.0}\"\n",
      "16\n",
      "Updating issue \"PP-407\" with \"{'summary': 'Implement extract_time_range method', 'description': 'see design', 'customfield_11701': '2021-03-24', 'customfield_11702': '2021-03-24', 'customfield_10004': 1.0}\"\n",
      "Updating issue \"PP-408\" with \"{'summary': 'Implement make_filter (from time range) method', 'description': 'see design', 'customfield_10004': 1.0}\"\n",
      "4\n",
      "Updating issue \"PP-409\" with \"{'summary': 'Extract benchmark_value in extract_relation_statistics method', 'description': 'see design', 'customfield_11701': '2021-03-25', 'customfield_11702': '2021-03-25', 'customfield_10004': 1.5}\"\n",
      "Updating issue \"PP-410\" with \"{'summary': 'Handle benchmark future argument in CaseSummaryQueryRunner', 'description': None, 'customfield_10004': 4.0}\"\n",
      "16\n",
      "Updating issue \"PP-411\" with \"{'summary': 'Integrate benchmark calculations with the caseSummaryCollection query', 'description': 'see design', 'customfield_10004': 8.0}\"\n",
      "32\n",
      "Updating issue \"PP-503\" with \"{'summary': 'Implement handling of packed nodes in calculate_benchmark (use decompose_packed_nodes decorator)', 'description': 'see design, implement a unit test for this use case', 'customfield_10004': 2.0}\"\n",
      "8\n",
      "Updating issue \"PP-460\" with \"{'summary': 'Node Expand and Collapse', 'description': 'see design'}\"\n",
      "None\n",
      "Updating issue \"PP-461\" with \"{'summary': 'Implement extract_action method', 'description': 'see design', 'customfield_10004': 2.0}\"\n",
      "8\n",
      "Updating issue \"PP-462\" with \"{'summary': 'Implement make_filter_from_node method', 'description': 'see design', 'customfield_10004': 2.0}\"\n",
      "8\n",
      "Updating issue \"PP-463\" with \"{'summary': 'Implement extract_filters method', 'description': 'extracting filters from list of actions', 'customfield_10004': 1.0}\"\n",
      "4\n",
      "Updating issue \"PP-464\" with \"{'summary': 'Integrate expand / collapse functionality with the caseSummaryCollection query', 'description': 'see design', 'customfield_10004': 8.0}\"\n",
      "32\n",
      "Updating issue \"PP-465\" with \"{'summary': 'find_child_nodes', 'description': 'see design'}\"\n",
      "None\n",
      "Updating issue \"PP-454\" with \"{'summary': 'Implement get_child_node_columns method', 'description': None, 'customfield_11701': '2021-03-17', 'customfield_11702': '2021-03-18', 'customfield_10004': 1.0}\"\n",
      "Updating issue \"PP-455\" with \"{'summary': 'Implement find_child_nodes method in PandasAnalyticalService, for now without sorting', 'description': 'implement first version without sorting or handing other options (dry_run, pagination, match_prefix)', 'customfield_11701': '2021-03-18', 'customfield_11702': '2021-03-18', 'customfield_10004': 1.625}\"\n",
      "Updating issue \"PP-456\" with \"{'summary': 'Handle sorting by case_count in find_child_nodes, add case_count property in Node class', 'description': 'see design, also set case_count property in returned nodes, for now we can allow only sorting by 1 sort criteria', 'customfield_11701': '2021-03-18', 'customfield_11702': '2021-03-19', 'customfield_10004': 1.25}\"\n",
      "Updating issue \"PP-466\" with \"{'summary': 'Handle sorting by options 2 - 7 in find_child_nodes', 'description': 'also handle sorting by multiple columns', 'customfield_11701': '2021-03-22', 'customfield_11702': '2021-03-23', 'customfield_10004': 2.0}\"\n",
      "Updating issue \"PP-467\" with \"{'summary': 'Handle sorting by rating in find_child_nodes', 'description': None, 'customfield_10004': 2.5}\"\n",
      "10\n",
      "Updating issue \"PP-468\" with \"{'summary': 'Handle sorting by name in find_child_nodes', 'description': None, 'customfield_11701': '2021-03-23', 'customfield_11702': '2021-03-23', 'customfield_10004': 1.75}\"\n",
      "Updating issue \"PP-469\" with \"{'summary': 'Handle pagination argument', 'description': 'Limit results to a specific page only', 'customfield_10004': 1.0}\"\n",
      "4\n",
      "Updating issue \"PP-470\" with \"{'summary': 'Handle match_prefix argument', 'description': None, 'customfield_10004': 2.0}\"\n",
      "8\n",
      "Updating issue \"PP-471\" with \"{'summary': 'Handle dry_run option', 'description': 'return int when dry_run = True', 'customfield_10004': 2.0}\"\n",
      "8\n",
      "Updating issue \"PP-472\" with \"{'summary': 'Sorting and Pagination of Nodes', 'description': 'see design'}\"\n",
      "None\n",
      "Updating issue \"PP-473\" with \"{'summary': 'Implement extract_sort_criteria method', 'description': None, 'customfield_11701': '2021-03-25', 'customfield_11702': '2021-03-26', 'customfield_10004': 1.25}\"\n",
      "Updating issue \"PP-474\" with \"{'summary': 'Implement pack_nodes method, add is_packed() and unpack() methods to Node class', 'description': 'see design, make sure to check performance for ~40k nodes', 'customfield_11701': '2021-03-25', 'customfield_11702': '2021-03-26', 'customfield_10004': 2.0}\"\n",
      "Updating issue \"PP-475\" with \"{'summary': 'Implement extract_pagination method', 'description': None, 'customfield_11701': '2021-03-26', 'customfield_11702': '2021-03-26', 'customfield_10004': 0.5}\"\n",
      "Updating issue \"PP-476\" with \"{'summary': 'Implement get_graph_orientation method', 'description': None, 'customfield_10004': 1.0}\"\n",
      "4\n",
      "Updating issue \"PP-477\" with \"{'summary': 'Implement extract_graph_pagination method', 'description': None, 'customfield_11701': '2021-03-29', 'customfield_10004': 1.0}\"\n",
      "4\n",
      "Updating issue \"PP-478\" with \"{'summary': 'Update group_nodes method in PandasAnalyticalService to allow a single packed node on either side (and up to 2 pack nodes in total)', 'description': 'see design', 'customfield_11701': '2021-03-26', 'customfield_10004': 8.0}\"\n",
      "32\n",
      "Updating issue \"PP-504\" with \"{'summary': 'Handle single packed node on each side in calculate_benchmark method', 'description': 'see also 1.14.6', 'customfield_10004': 6.5}\"\n",
      "26\n",
      "Updating issue \"PP-479\" with \"{'summary': 'Implement decompose_packed_nodes decorator', 'description': 'Apply decorator to all group_nodes implementations (see design), make sure to run queries in parallel', 'customfield_10004': 2.0}\"\n",
      "8\n",
      "Updating issue \"PP-480\" with \"{'summary': 'Refactor make_abstract_graph_query to use find_child_nodes method', 'description': 'pass sorting criteria, see updated design, make sure to run lhs/rhs queries in parallel', 'customfield_10004': 6.0}\"\n",
      "24\n",
      "Updating issue \"PP-481\" with \"{'summary': 'Refactor AbstractGraphModel class and make_abstract_graph_model method to reflect design', 'description': 'make sure to pack nodes according to rules, see design', 'customfield_11701': '2021-03-29', 'customfield_10004': 4.0}\"\n",
      "16\n",
      "Updating issue \"PP-482\" with \"{'summary': 'Update caseSummaryCollection endpoint to handle sorting and pagination', 'description': 'make sure sorting and pagination rules are processed, add unit tests for this functionality', 'customfield_10004': 2.0}\"\n",
      "8\n",
      "Updating issue \"PP-483\" with \"{'summary': 'allDatasets endpoint', 'description': 'see design'}\"\n",
      "None\n",
      "Updating issue \"PP-505\" with \"{'summary': 'define the DatasetDetails class', 'description': None, 'customfield_11701': '2021-03-24', 'customfield_11702': '2021-03-24', 'customfield_10004': 0.25}\"\n",
      "Updating issue \"PP-484\" with \"{'summary': 'Implement get_all_datasets method in the DatasetService class', 'description': None, 'customfield_10004': 4.0}\"\n",
      "16\n",
      "ERROR: It is not a range:  ['DM']\n",
      "Updating issue \"PP-506\" with \"{'summary': 'Implement format_all_datasets_response method', 'description': None, 'customfield_10004': 0.25}\"\n",
      "1\n",
      "Updating issue \"PP-518\" with \"{'summary': 'Implement DummyDatasetService', 'description': 'This is to run P2 application without the actual DM implementation yet, do not implement any unit tests for this class', 'customfield_11701': '2021-03-25', 'customfield_11702': '2021-03-25', 'customfield_10004': 1.0}\"\n",
      "Updating issue \"PP-507\" with \"{'summary': 'Implement allDatasets endpoint', 'description': 'use DummyDatasetService for now', 'customfield_11701': '2021-03-26', 'customfield_11702': '2021-03-26', 'customfield_10004': 1.0}\"\n",
      "Updating issue \"PP-508\" with \"{'summary': 'getLDSDatasetsIDs endpoint', 'description': 'see design'}\"\n",
      "None\n",
      "Updating issue \"PP-509\" with \"{'summary': 'Implement getLDSDatasetsIDs endpoint', 'description': 'use mock DatasetService for now', 'customfield_10004': 1.0}\"\n",
      "4\n",
      "Updating issue \"PP-510\" with \"{'summary': 'dataset endpoint', 'description': 'see design'}\"\n",
      "None\n",
      "Updating issue \"PP-511\" with \"{'summary': 'Implement get_datasets method in DatasetService', 'description': '1.15.2', 'customfield_10004': 1.0}\"\n",
      "4\n",
      "ERROR: It is not a range:  ['DM']\n",
      "Updating issue \"PP-512\" with \"{'summary': 'Implement get_pipeline_statistics method in DatasetService', 'description': 'connect to the pipeline database', 'customfield_10004': 6.0}\"\n",
      "24\n",
      "ERROR: It is not a range:  ['O2Grouper']\n",
      "Updating issue \"PP-513\" with \"{'summary': 'Implement dataset endpoint', 'description': None, 'customfield_10004': 2.0}\"\n",
      "8\n",
      "Updating issue \"PP-514\" with \"{'summary': 'caseStatistics endpoint', 'description': 'see design'}\"\n",
      "None\n",
      "Updating issue \"PP-519\" with \"{'summary': 'Implement get_case_statistics in PandasAnalyticalService', 'description': 'Handle sorting and limitation of results', 'customfield_10004': 8.0}\"\n",
      "32\n",
      "Updating issue \"PP-520\" with \"{'summary': 'Implement get_serviceline_benchmarks in PandasAnalyticalService', 'description': 'similar to 1.19.2', 'customfield_10004': 4.0}\"\n",
      "16\n",
      "Updating issue \"PP-521\" with \"{'summary': 'Implement join_serviceline_benchmarks', 'description': 'similar to 1.19.3', 'customfield_10004': 1.0}\"\n",
      "4\n",
      "Updating issue \"PP-522\" with \"{'summary': 'Implement format_case_statistics', 'description': 'Prepare synthetic data frames for unit tests (without and with benchmark values)', 'customfield_10004': 2.0}\"\n",
      "8\n",
      "Updating issue \"PP-523\" with \"{'summary': 'Implement caseStatistics endpoint', 'description': 'see design, make sure to run queries in parallel', 'customfield_10004': 5.0}\"\n",
      "20\n",
      "Updating issue \"PP-515\" with \"{'summary': 'caseSummaryCollectionMapView endpoint', 'description': 'see design'}\"\n",
      "None\n",
      "Updating issue \"PP-524\" with \"{'summary': 'Implement get_map_view_entity_statistics in PandasAnalyticalService', 'description': None, 'customfield_10004': 5.0}\"\n",
      "20\n",
      "Updating issue \"PP-525\" with \"{'summary': 'Implement get_careunit_benchmarks in PandasAnalyticalService', 'description': None, 'customfield_10004': 5.0}\"\n",
      "20\n",
      "Updating issue \"PP-526\" with \"{'summary': 'Implement join_careunit_benchmarks', 'description': 'Prepare sythetic data frames for the unit test', 'customfield_10004': 1.0}\"\n",
      "4\n",
      "Updating issue \"PP-527\" with \"{'summary': 'Implement format_entity_statistics', 'description': 'Prepare synthetic data frames for unit tests (without and with benchmark values)', 'customfield_10004': 1.0}\"\n",
      "4\n",
      "Updating issue \"PP-528\" with \"{'summary': 'Implement caseSummaryCollectionMapView endpoint', 'description': 'see design, make sure to run queries in parallel', 'customfield_10004': 3.0}\"\n",
      "12\n",
      "Updating issue \"PP-529\" with \"{'summary': 'Error Handling', 'description': 'see design'}\"\n",
      "None\n",
      "Updating issue \"PP-530\" with \"{'summary': 'Define ApplicationError base class and app_error_class decorator', 'description': 'just copy from the design and add unit tests', 'customfield_10004': 0.5}\"\n",
      "2\n",
      "Updating issue \"PP-531\" with \"{'summary': \"Review applicatoin's code, find all raise statements and throw proper errors\", 'description': 'Declare proper exception classes, define error variables, use DatasetNotFound as a model example, place all errors in the errors.py module', 'customfield_10004': 9.0}\"\n",
      "36\n",
      "Updating issue \"PP-532\" with \"{'summary': 'Add list of error codes and descriptions to technical design', 'description': 'This task is development driven', 'customfield_10004': 1.0}\"\n",
      "4\n",
      "Updating issue \"PP-533\" with \"{'summary': 'Backend Configuration', 'description': 'see design'}\"\n",
      "None\n",
      "Updating issue \"PP-534\" with \"{'summary': 'Implement AppConfig', 'description': 'Define ConfigVariables and defaults, validate configuration on first access to AppConfig'}\"\n",
      "None\n",
      "Updating issue \"PP-537\" with \"{'summary': 'Data Suppression (LDS-11 rule)', 'description': None}\"\n",
      "None\n",
      "Updating issue \"PP-538\" with \"{'summary': 'User Dataset Upload', 'description': None}\"\n",
      "None\n",
      "Updating issue \"PP-539\" with \"{'summary': 'Snowflake Integration', 'description': None}\"\n",
      "None\n",
      "Updating issue \"PP-540\" with \"{'summary': 'ServiceLine API', 'description': 'Move code and data from existing P2v2 implementation to P2v3'}\"\n",
      "None\n",
      "Updating issue \"PP-541\" with \"{'summary': 'Implement searchServiceLine endpoint', 'description': None}\"\n",
      "None\n",
      "Creating issue \"Implement searchAllDiagnose endpoint\"\n",
      "{'id': '62932', 'key': 'PP-580', 'self': 'https://tangramcare.atlassian.net/rest/api/2/issue/62932'}\n",
      "Creating issue \"Implement filteredServiceLines endpoint\"\n",
      "{'id': '62933', 'key': 'PP-581', 'self': 'https://tangramcare.atlassian.net/rest/api/2/issue/62933'}\n",
      "Creating issue \"Implement filteredClinicalEpisodes endpoint\"\n",
      "{'id': '62934', 'key': 'PP-582', 'self': 'https://tangramcare.atlassian.net/rest/api/2/issue/62934'}\n",
      "Creating issue \"Implement filteredDRGs endpoint\"\n",
      "{'id': '62935', 'key': 'PP-583', 'self': 'https://tangramcare.atlassian.net/rest/api/2/issue/62935'}\n",
      "Updating issue \"PP-542\" with \"{'summary': 'Location API', 'description': 'Move code and data from existing P2v2 implementation to P2v3'}\"\n",
      "None\n",
      "Updating issue \"PP-543\" with \"{'summary': 'Move allUSStates endpoint', 'description': 'Copy data to postgres, move django model from P2v2', 'customfield_10004': 1.0}\"\n",
      "4\n",
      "Creating issue \"Move usState endpoint\"\n",
      "{'id': '62936', 'key': 'PP-584', 'self': 'https://tangramcare.atlassian.net/rest/api/2/issue/62936'}\n",
      "Creating issue \"Move searchUsLocation endpoint\"\n",
      "{'id': '62937', 'key': 'PP-585', 'self': 'https://tangramcare.atlassian.net/rest/api/2/issue/62937'}\n",
      "Creating issue \"Move usStateGeoAll endpoint\"\n",
      "{'id': '62938', 'key': 'PP-586', 'self': 'https://tangramcare.atlassian.net/rest/api/2/issue/62938'}\n",
      "Creating issue \"Move usCountyGeoForState endpoint\"\n",
      "{'id': '62939', 'key': 'PP-587', 'self': 'https://tangramcare.atlassian.net/rest/api/2/issue/62939'}\n",
      "Creating issue \"Move usStateGeoSearch\"\n",
      "{'id': '62940', 'key': 'PP-588', 'self': 'https://tangramcare.atlassian.net/rest/api/2/issue/62940'}\n",
      "Creating issue \"Move usCountyGeoAll endpoint\"\n",
      "{'id': '62941', 'key': 'PP-589', 'self': 'https://tangramcare.atlassian.net/rest/api/2/issue/62941'}\n",
      "Creating issue \"Move locationByRadialLocation endpoint\"\n",
      "{'id': '62942', 'key': 'PP-590', 'self': 'https://tangramcare.atlassian.net/rest/api/2/issue/62942'}\n",
      "Updating issue \"PP-266\" with \"{'summary': 'Testing caseSummaryCollection query', 'description': None}\"\n",
      "None\n",
      "Updating issue \"PP-267\" with \"{'summary': 'Test caseSummaryCollection - market share graph', 'description': 'Test filters and responsed for caseSummaryCollection with benchmark option set to false'}\"\n",
      "None\n",
      "Updating issue \"PP-268\" with \"{'summary': 'Test caseSummaryCollection - benchmark graph', 'description': 'Test filters and responses for caseSummaryCollection with benchmark set to true'}\"\n",
      "None\n",
      "Updating issue \"PP-269\" with \"{'summary': 'Test caseSummaryCollection - 2D graph', 'description': 'Test filters and responses for caseSummaryCollection with 2D graph benchmark settings'}\"\n",
      "None\n",
      "Updating issue \"PP-356\" with \"{'summary': 'Report View Management', 'description': None}\"\n",
      "None\n",
      "Updating issue \"PP-357\" with \"{'summary': 'Environment Setup', 'description': 'Setu development environment including DJangoand PGSql', 'customfield_11701': '2021-03-05', 'customfield_11702': '2021-03-16', 'customfield_10004': 5.0}\"\n",
      "Updating issue \"PP-358\" with \"{'summary': 'Define schema.py', 'description': 'Class that defines graphQL queries, endpoints and object structure. Python file will have definition of following ojects. Developer can refer existing P2BE code at p2backend\\\\ReportView\\\\schema.py file.', 'customfield_11701': '2021-03-05', 'customfield_11702': '2021-03-16', 'customfield_10004': 1.0}\"\n",
      "Updating issue \"PP-359\" with \"{'summary': 'Define reportview_api.py', 'description': 'Class that has actual implementation for all end points. Developer can refer p2backend\\\\BigSense\\\\Reportview\\\\reportview_api.py file from existing P2BE code', 'customfield_11701': '2021-03-05', 'customfield_11702': '2021-03-16', 'customfield_10004': 1.0}\"\n",
      "Updating issue \"PP-360\" with \"{'summary': 'Define model.py', 'description': 'Class that defines the Entity Relation of report view as per OLTP database(Postgres)', 'customfield_11701': '2021-03-05', 'customfield_11702': '2021-03-16', 'customfield_10004': 1.0}\"\n",
      "Updating issue \"PP-361\" with \"{'summary': 'Implementing Endpoints', 'description': 'This implementation will have a resolve method in schema.py and corresponding method in reportview_api.py file', 'customfield_11701': '2021-03-05', 'customfield_11702': '2021-03-16', 'customfield_10004': 1.0}\"\n",
      "Updating issue \"PP-362\" with \"{'summary': 'ReportView', 'description': None, 'customfield_11701': '2021-03-05', 'customfield_11702': '2021-03-16', 'customfield_10004': 1.0}\"\n",
      "Updating issue \"PP-363\" with \"{'summary': 'ReportViews', 'description': None, 'customfield_11701': '2021-03-05', 'customfield_11702': '2021-03-16', 'customfield_10004': 1.0}\"\n",
      "Updating issue \"PP-364\" with \"{'summary': 'SearchReportViews', 'description': None, 'customfield_11701': '2021-03-05', 'customfield_11702': '2021-03-16', 'customfield_10004': 1.0}\"\n",
      "Updating issue \"PP-365\" with \"{'summary': 'CreateReportView', 'description': None, 'customfield_11701': '2021-03-05', 'customfield_11702': '2021-03-16', 'customfield_10004': 1.0}\"\n",
      "Updating issue \"PP-366\" with \"{'summary': 'UpdateReportView', 'description': None, 'customfield_11701': '2021-03-05', 'customfield_11702': '2021-03-16', 'customfield_10004': 1.0}\"\n",
      "Updating issue \"PP-367\" with \"{'summary': 'DeleteReportView', 'description': None, 'customfield_11701': '2021-03-05', 'customfield_11702': '2021-03-16', 'customfield_10004': 1.0}\"\n",
      "Updating issue \"PP-516\" with \"{'summary': '[BE] Extract user details from Auth0 token', 'description': None}\"\n",
      "None\n",
      "Updating issue \"PP-517\" with \"{'summary': '[BE] Modify Report view endpoints to use user details from Auth0 token', 'description': None}\"\n",
      "None\n",
      "Updating issue \"PP-567\" with \"{'summary': 'Integrate analytical endpoints with django', 'description': 'Integrate endpoints with django. To be seen under https://p2v3.ownedoutcomes.com/backend/graphql/ url.\\n\\nList:caseLocationDetails endpoint\\n\\ncaseLobCounts endpoint\\n\\ngetAvailableYears endpoint\\n\\ngetDateTimeStatsForListOfYears endpoint\\n\\nfilteredCareUnitTypes endpoint\\n\\ngetDatasetsInitialFilters\\n\\ngetDateTimesForYear endpoint\\n\\nallDatasets endpoint\\n\\ngetLDSDatasetsIDs endpoint', 'customfield_10004': 2.0}\"\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "create_or_update_issues(jira=jira, sheet=sheet, schema=schema, project='PP', label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "workbook.save(filename=filename)\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'wbs': '0', 'name': 'Project configuration', 'description': 'Create a project from a template and add library requirements', 'dependency': 'None', 'estimated_development_time_designer': '1.5', 'estimated_development_time_reviewer': '1', 'estimated_development_time_pm': '1', 'assigned_developer': 'None', 'estimated_testing_time_tester': 'None', 'estimated_testing_time_pm': 'None', 'assigned_tester': 'None', 'actual_development_time_start': 'None', 'actual_development_time_finish': 'None', 'actual_development_time_duration': '1.5', 'commit_statistics_insertions': 'None', 'commit_statistics_deletions': 'None', 'code_documentation_time': 'None', 'jira_id': 'PP-458', 'commit_id': 'None', 'sprint': 'None', 'comment': 'None'}, {'wbs': '1', 'name': 'Describing abstract classes', 'description': 'Describe abstract classes and helper classes (like classes in model.py) ', 'dependency': '0', 'estimated_development_time_designer': '1', 'estimated_development_time_reviewer': 'None', 'estimated_development_time_pm': 'None', 'assigned_developer': 'None', 'estimated_testing_time_tester': 'None', 'estimated_testing_time_pm': 'None', 'assigned_tester': 'None', 'actual_development_time_start': 'None', 'actual_development_time_finish': 'None', 'actual_development_time_duration': 'None', 'commit_statistics_insertions': 'None', 'commit_statistics_deletions': 'None', 'code_documentation_time': 'None', 'jira_id': 'PP-485', 'commit_id': 'None', 'sprint': 'None', 'comment': 'None'}, {'wbs': '2', 'name': 'DatasetStorage mock implementation', 'description': 'Implement mock implementation of DatasetStorage for use in tests (documentation).', 'dependency': '0, 1', 'estimated_development_time_designer': '2', 'estimated_development_time_reviewer': '2', 'estimated_development_time_pm': '4', 'assigned_developer': 'None', 'estimated_testing_time_tester': 'None', 'estimated_testing_time_pm': 'None', 'assigned_tester': 'None', 'actual_development_time_start': 'None', 'actual_development_time_finish': 'None', 'actual_development_time_duration': '2', 'commit_statistics_insertions': 'None', 'commit_statistics_deletions': 'None', 'code_documentation_time': 'None', 'jira_id': 'PP-311', 'commit_id': 'None', 'sprint': 'None', 'comment': 'None'}, {'wbs': '3', 'name': 'Access mock implementation', 'description': 'Implement mock implementation of Access for use in tests (documentation).', 'dependency': '0, 1', 'estimated_development_time_designer': '4', 'estimated_development_time_reviewer': '4', 'estimated_development_time_pm': '4', 'assigned_developer': 'None', 'estimated_testing_time_tester': 'None', 'estimated_testing_time_pm': 'None', 'assigned_tester': 'None', 'actual_development_time_start': 'None', 'actual_development_time_finish': 'None', 'actual_development_time_duration': 'None', 'commit_statistics_insertions': 'None', 'commit_statistics_deletions': 'None', 'code_documentation_time': 'None', 'jira_id': 'PP-312', 'commit_id': 'None', 'sprint': 'None', 'comment': 'None'}, {'wbs': '4', 'name': 'DatasetManager implementation', 'description': 'implement the final version of the class according to the documentation (only class with methods without graphql)', 'dependency': '2, 3', 'estimated_development_time_designer': 'None', 'estimated_development_time_reviewer': 'None', 'estimated_development_time_pm': 'None', 'assigned_developer': 'None', 'estimated_testing_time_tester': 'None', 'estimated_testing_time_pm': 'None', 'assigned_tester': 'None', 'actual_development_time_start': 'None', 'actual_development_time_finish': 'None', 'actual_development_time_duration': 'None', 'commit_statistics_insertions': 'None', 'commit_statistics_deletions': 'None', 'code_documentation_time': 'None', 'jira_id': 'PP-313', 'commit_id': 'None', 'sprint': 'None', 'comment': 'None'}, {'wbs': '4.1', 'name': 'Implement get()', 'description': 'implement method according to the documentation', 'dependency': 'None', 'estimated_development_time_designer': '5', 'estimated_development_time_reviewer': '6', 'estimated_development_time_pm': '8', 'assigned_developer': 'None', 'estimated_testing_time_tester': 'None', 'estimated_testing_time_pm': 'None', 'assigned_tester': 'None', 'actual_development_time_start': 'None', 'actual_development_time_finish': 'None', 'actual_development_time_duration': 'None', 'commit_statistics_insertions': 'None', 'commit_statistics_deletions': 'None', 'code_documentation_time': 'None', 'jira_id': 'PP-314', 'commit_id': 'None', 'sprint': 'None', 'comment': 'None'}, {'wbs': '4.2', 'name': 'Implement share()', 'description': 'implement method according to the documentation', 'dependency': 'None', 'estimated_development_time_designer': '1.5', 'estimated_development_time_reviewer': '1.5', 'estimated_development_time_pm': '2', 'assigned_developer': 'None', 'estimated_testing_time_tester': 'None', 'estimated_testing_time_pm': 'None', 'assigned_tester': 'None', 'actual_development_time_start': 'None', 'actual_development_time_finish': 'None', 'actual_development_time_duration': 'None', 'commit_statistics_insertions': 'None', 'commit_statistics_deletions': 'None', 'code_documentation_time': 'None', 'jira_id': 'PP-315', 'commit_id': 'None', 'sprint': 'None', 'comment': 'None'}, {'wbs': '4.3', 'name': 'Implement unshare()', 'description': 'implement method according to the documentation', 'dependency': 'None', 'estimated_development_time_designer': '1.5', 'estimated_development_time_reviewer': '1.5', 'estimated_development_time_pm': '2', 'assigned_developer': 'None', 'estimated_testing_time_tester': 'None', 'estimated_testing_time_pm': 'None', 'assigned_tester': 'None', 'actual_development_time_start': 'None', 'actual_development_time_finish': 'None', 'actual_development_time_duration': 'None', 'commit_statistics_insertions': 'None', 'commit_statistics_deletions': 'None', 'code_documentation_time': 'None', 'jira_id': 'PP-316', 'commit_id': 'None', 'sprint': 'None', 'comment': 'None'}, {'wbs': '4.4', 'name': 'Implement register()', 'description': 'implement method according to the documentation', 'dependency': 'None', 'estimated_development_time_designer': '1.5', 'estimated_development_time_reviewer': '1.5', 'estimated_development_time_pm': '2', 'assigned_developer': 'None', 'estimated_testing_time_tester': 'None', 'estimated_testing_time_pm': 'None', 'assigned_tester': 'None', 'actual_development_time_start': 'None', 'actual_development_time_finish': 'None', 'actual_development_time_duration': 'None', 'commit_statistics_insertions': 'None', 'commit_statistics_deletions': 'None', 'code_documentation_time': 'None', 'jira_id': 'PP-317', 'commit_id': 'None', 'sprint': 'None', 'comment': 'None'}, {'wbs': '4.5', 'name': 'Implement remove()', 'description': 'implement method according to the documentation', 'dependency': 'None', 'estimated_development_time_designer': '1.5', 'estimated_development_time_reviewer': '1.5', 'estimated_development_time_pm': '2', 'assigned_developer': 'None', 'estimated_testing_time_tester': 'None', 'estimated_testing_time_pm': 'None', 'assigned_tester': 'None', 'actual_development_time_start': 'None', 'actual_development_time_finish': 'None', 'actual_development_time_duration': 'None', 'commit_statistics_insertions': 'None', 'commit_statistics_deletions': 'None', 'code_documentation_time': 'None', 'jira_id': 'PP-318', 'commit_id': 'None', 'sprint': 'None', 'comment': 'None'}, {'wbs': '4.6', 'name': 'Implement add_user()', 'description': 'implement method according to the documentation', 'dependency': 'None', 'estimated_development_time_designer': '1.5', 'estimated_development_time_reviewer': '1.5', 'estimated_development_time_pm': '2', 'assigned_developer': 'None', 'estimated_testing_time_tester': 'None', 'estimated_testing_time_pm': 'None', 'assigned_tester': 'None', 'actual_development_time_start': 'None', 'actual_development_time_finish': 'None', 'actual_development_time_duration': 'None', 'commit_statistics_insertions': 'None', 'commit_statistics_deletions': 'None', 'code_documentation_time': 'None', 'jira_id': 'PP-486', 'commit_id': 'None', 'sprint': 'None', 'comment': 'None'}, {'wbs': '4.7', 'name': 'Implement add_parameter()', 'description': 'implement method according to the documentation', 'dependency': 'None', 'estimated_development_time_designer': '1.5', 'estimated_development_time_reviewer': '1.5', 'estimated_development_time_pm': '2', 'assigned_developer': 'None', 'estimated_testing_time_tester': 'None', 'estimated_testing_time_pm': 'None', 'assigned_tester': 'None', 'actual_development_time_start': 'None', 'actual_development_time_finish': 'None', 'actual_development_time_duration': 'None', 'commit_statistics_insertions': 'None', 'commit_statistics_deletions': 'None', 'code_documentation_time': 'None', 'jira_id': 'PP-487', 'commit_id': 'None', 'sprint': 'None', 'comment': 'None'}, {'wbs': '5', 'name': 'GraphQL server implementation', 'description': 'None', 'dependency': '4', 'estimated_development_time_designer': 'None', 'estimated_development_time_reviewer': 'None', 'estimated_development_time_pm': 'None', 'assigned_developer': 'None', 'estimated_testing_time_tester': 'None', 'estimated_testing_time_pm': 'None', 'assigned_tester': 'None', 'actual_development_time_start': 'None', 'actual_development_time_finish': 'None', 'actual_development_time_duration': 'None', 'commit_statistics_insertions': 'None', 'commit_statistics_deletions': 'None', 'code_documentation_time': 'None', 'jira_id': 'PP-319', 'commit_id': 'None', 'sprint': 'None', 'comment': 'None'}, {'wbs': '5.1', 'name': 'Create model', 'description': 'You have to create a model the chosen technology (graphene) and pass all the requiered objects to it. ', 'dependency': 'None', 'estimated_development_time_designer': '4', 'estimated_development_time_reviewer': '6', 'estimated_development_time_pm': '8', 'assigned_developer': 'None', 'estimated_testing_time_tester': 'None', 'estimated_testing_time_pm': 'None', 'assigned_tester': 'None', 'actual_development_time_start': 'None', 'actual_development_time_finish': 'None', 'actual_development_time_duration': 'None', 'commit_statistics_insertions': 'None', 'commit_statistics_deletions': 'None', 'code_documentation_time': 'None', 'jira_id': 'PP-320', 'commit_id': 'None', 'sprint': 'None', 'comment': 'None'}, {'wbs': '5.2', 'name': 'Implement get() endpoint', 'description': 'create a graphql endpoint according to the documentation with filters', 'dependency': '5.1', 'estimated_development_time_designer': '5', 'estimated_development_time_reviewer': '6', 'estimated_development_time_pm': '8', 'assigned_developer': 'None', 'estimated_testing_time_tester': 'None', 'estimated_testing_time_pm': 'None', 'assigned_tester': 'None', 'actual_development_time_start': 'None', 'actual_development_time_finish': 'None', 'actual_development_time_duration': 'None', 'commit_statistics_insertions': 'None', 'commit_statistics_deletions': 'None', 'code_documentation_time': 'None', 'jira_id': 'PP-321', 'commit_id': 'None', 'sprint': 'None', 'comment': 'None'}, {'wbs': '5.3', 'name': 'Implement share() endpoint', 'description': 'create a graphql endpoint according to the documentation', 'dependency': '5.1', 'estimated_development_time_designer': '2', 'estimated_development_time_reviewer': '2', 'estimated_development_time_pm': '2', 'assigned_developer': 'None', 'estimated_testing_time_tester': 'None', 'estimated_testing_time_pm': 'None', 'assigned_tester': 'None', 'actual_development_time_start': 'None', 'actual_development_time_finish': 'None', 'actual_development_time_duration': 'None', 'commit_statistics_insertions': 'None', 'commit_statistics_deletions': 'None', 'code_documentation_time': 'None', 'jira_id': 'PP-322', 'commit_id': 'None', 'sprint': 'None', 'comment': 'None'}, {'wbs': '5.4', 'name': 'Implement unshare() endpoint', 'description': 'create a graphql endpoint according to the documentation', 'dependency': '5.1', 'estimated_development_time_designer': '2', 'estimated_development_time_reviewer': '2', 'estimated_development_time_pm': '2', 'assigned_developer': 'None', 'estimated_testing_time_tester': 'None', 'estimated_testing_time_pm': 'None', 'assigned_tester': 'None', 'actual_development_time_start': 'None', 'actual_development_time_finish': 'None', 'actual_development_time_duration': 'None', 'commit_statistics_insertions': 'None', 'commit_statistics_deletions': 'None', 'code_documentation_time': 'None', 'jira_id': 'PP-323', 'commit_id': 'None', 'sprint': 'None', 'comment': 'None'}, {'wbs': '5.5', 'name': 'Implement register() endpoint', 'description': 'create a graphql endpoint according to the documentation', 'dependency': '5.1', 'estimated_development_time_designer': '2', 'estimated_development_time_reviewer': '2', 'estimated_development_time_pm': '2', 'assigned_developer': 'None', 'estimated_testing_time_tester': 'None', 'estimated_testing_time_pm': 'None', 'assigned_tester': 'None', 'actual_development_time_start': 'None', 'actual_development_time_finish': 'None', 'actual_development_time_duration': 'None', 'commit_statistics_insertions': 'None', 'commit_statistics_deletions': 'None', 'code_documentation_time': 'None', 'jira_id': 'PP-324', 'commit_id': 'None', 'sprint': 'None', 'comment': 'None'}, {'wbs': '5.6', 'name': 'Implement remove() endpoint', 'description': 'create a graphql endpoint according to the documentation', 'dependency': '5.1', 'estimated_development_time_designer': '2', 'estimated_development_time_reviewer': '2', 'estimated_development_time_pm': '2', 'assigned_developer': 'None', 'estimated_testing_time_tester': 'None', 'estimated_testing_time_pm': 'None', 'assigned_tester': 'None', 'actual_development_time_start': 'None', 'actual_development_time_finish': 'None', 'actual_development_time_duration': 'None', 'commit_statistics_insertions': 'None', 'commit_statistics_deletions': 'None', 'code_documentation_time': 'None', 'jira_id': 'PP-325', 'commit_id': 'None', 'sprint': 'None', 'comment': 'None'}, {'wbs': '5.7', 'name': 'Implement add_user() endpoint', 'description': 'create a graphql endpoint according to the documentation', 'dependency': '5.1', 'estimated_development_time_designer': '2', 'estimated_development_time_reviewer': '2', 'estimated_development_time_pm': '2', 'assigned_developer': 'None', 'estimated_testing_time_tester': 'None', 'estimated_testing_time_pm': 'None', 'assigned_tester': 'None', 'actual_development_time_start': 'None', 'actual_development_time_finish': 'None', 'actual_development_time_duration': 'None', 'commit_statistics_insertions': 'None', 'commit_statistics_deletions': 'None', 'code_documentation_time': 'None', 'jira_id': 'PP-488', 'commit_id': 'None', 'sprint': 'None', 'comment': 'None'}, {'wbs': '5.8', 'name': 'Implement add_parameter() endpoint', 'description': 'create a graphql endpoint according to the documentation', 'dependency': '5.1', 'estimated_development_time_designer': '2', 'estimated_development_time_reviewer': '2', 'estimated_development_time_pm': '2', 'assigned_developer': 'None', 'estimated_testing_time_tester': 'None', 'estimated_testing_time_pm': 'None', 'assigned_tester': 'None', 'actual_development_time_start': 'None', 'actual_development_time_finish': 'None', 'actual_development_time_duration': 'None', 'commit_statistics_insertions': 'None', 'commit_statistics_deletions': 'None', 'code_documentation_time': 'None', 'jira_id': 'PP-489', 'commit_id': 'None', 'sprint': 'None', 'comment': 'None'}, {'wbs': '6', 'name': 'Access on SQLAlchemy implementation', 'description': 'create models in sqlalchemy and implement abstract methods from access class.Create a model according to sql queries', 'dependency': '3', 'estimated_development_time_designer': '8', 'estimated_development_time_reviewer': '8', 'estimated_development_time_pm': '12', 'assigned_developer': 'None', 'estimated_testing_time_tester': 'None', 'estimated_testing_time_pm': 'None', 'assigned_tester': 'None', 'actual_development_time_start': 'None', 'actual_development_time_finish': 'None', 'actual_development_time_duration': 'None', 'commit_statistics_insertions': 'None', 'commit_statistics_deletions': 'None', 'code_documentation_time': 'None', 'jira_id': 'PP-326', 'commit_id': 'None', 'sprint': 'None', 'comment': 'None'}, {'wbs': '7', 'name': 'Admin panel implementation', 'description': 'Implement the admin panel using the Flask-Admin library', 'dependency': '5', 'estimated_development_time_designer': '6', 'estimated_development_time_reviewer': '8', 'estimated_development_time_pm': '16', 'assigned_developer': 'None', 'estimated_testing_time_tester': 'None', 'estimated_testing_time_pm': 'None', 'assigned_tester': 'None', 'actual_development_time_start': 'None', 'actual_development_time_finish': 'None', 'actual_development_time_duration': 'None', 'commit_statistics_insertions': 'None', 'commit_statistics_deletions': 'None', 'code_documentation_time': 'None', 'jira_id': 'PP-490', 'commit_id': 'None', 'sprint': 'None', 'comment': 'None'}, {'wbs': '8', 'name': 'Flask integration', 'description': 'Integrate the flask server with sqlalchemy and graphene ', 'dependency': '5, 6', 'estimated_development_time_designer': '4', 'estimated_development_time_reviewer': '6', 'estimated_development_time_pm': '8', 'assigned_developer': 'None', 'estimated_testing_time_tester': 'None', 'estimated_testing_time_pm': 'None', 'assigned_tester': 'None', 'actual_development_time_start': 'None', 'actual_development_time_finish': 'None', 'actual_development_time_duration': 'None', 'commit_statistics_insertions': 'None', 'commit_statistics_deletions': 'None', 'code_documentation_time': 'None', 'jira_id': 'PP-491', 'commit_id': 'None', 'sprint': 'None', 'comment': 'None'}, {'wbs': '9', 'name': 'DatasetStorage implementation', 'description': 'Implement implementation of DatasetStoage with selected warehouse (snowflake)', 'dependency': '2', 'estimated_development_time_designer': '4', 'estimated_development_time_reviewer': '4', 'estimated_development_time_pm': '8', 'assigned_developer': 'None', 'estimated_testing_time_tester': 'None', 'estimated_testing_time_pm': 'None', 'assigned_tester': 'None', 'actual_development_time_start': 'None', 'actual_development_time_finish': 'None', 'actual_development_time_duration': 'None', 'commit_statistics_insertions': 'None', 'commit_statistics_deletions': 'None', 'code_documentation_time': 'None', 'jira_id': 'PP-492', 'commit_id': 'None', 'sprint': 'None', 'comment': 'None'}]\n"
     ]
    }
   ],
   "source": [
    "map = {\n",
    "    \"A\": \"wbs\",\n",
    "    \"B\": \"name\",\n",
    "    \"C\": \"description\",\n",
    "    \"D\": \"dependency\",\n",
    "    \"E\": \"estimated_development_time_designer\",\n",
    "    \"F\": \"estimated_development_time_reviewer\",\n",
    "    \"G\": \"estimated_development_time_pm\",\n",
    "    \"H\": \"assigned_developer\",\n",
    "    \"I\": \"estimated_testing_time_tester\",\n",
    "    \"J\": \"estimated_testing_time_pm\",\n",
    "    \"K\": \"assigned_tester\",\n",
    "    \"L\": \"actual_development_time_start\",\n",
    "    \"M\": \"actual_development_time_finish\",\n",
    "    \"N\": \"actual_development_time_duration\",\n",
    "    \"O\": \"commit_statistics_insertions\",\n",
    "    \"P\": \"commit_statistics_deletions\",\n",
    "    \"Q\": \"code_documentation_time\",\n",
    "    \"R\": \"jira_id\",\n",
    "    \"S\": \"commit_id\",\n",
    "    \"T\": \"sprint\",\n",
    "    \"U\": \"comment\"\n",
    "}\n",
    "\n",
    "tasks_to_json = []\n",
    "\n",
    "i = schema['data_offset_row']\n",
    "while sheet[schema['columns']['WBS'] + str(i)].value is not None:\n",
    "    if len(str(sheet[schema['columns']['Name'] + str(i)].value)) != 0:\n",
    "        task = {}\n",
    "        for column in map:\n",
    "            task[map[column]] = str(sheet[column + str(i)].value)\n",
    "        tasks_to_json.append(task)\n",
    "    i = i + 1 \n",
    "\n",
    "print(tasks_to_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'213'"
      ]
     },
     "metadata": {},
     "execution_count": 217
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "execution_count": 248
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Creating issue \"Assign categories to View Dashboard testcases\"\n",
      "PP-160\n",
      "Link View Dashboard testcases\n",
      "Creating issue \"Assign categories to Graph View testcases\"\n",
      "PP-160\n",
      "Link Graph View testcases\n",
      "Creating issue \"Assign categories to Map View testcases\"\n",
      "PP-160\n",
      "Link Map View testcases\n",
      "Creating issue \"Assign categories to 2D Graph View testcases\"\n",
      "PP-160\n",
      "Link 2D Graph View testcases\n",
      "Creating issue \"Assign categories to Authentication testcases\"\n",
      "PP-160\n",
      "Link Authentication testcases\n",
      "Creating issue \"Assign categories to General Sidebar Interactions testcases\"\n",
      "PP-160\n",
      "Link General Sidebar Interactions testcases\n",
      "Creating issue \"Assign categories to Dataset Management testcases\"\n",
      "PP-160\n",
      "Link Dataset Management testcases\n",
      "Creating issue \"Assign categories to Location Filter testcases\"\n",
      "PP-160\n",
      "Link Location Filter testcases\n",
      "Creating issue \"Assign categories to Service Line Filter testcases\"\n",
      "PP-160\n",
      "Link Service Line Filter testcases\n",
      "Creating issue \"Assign categories to Entity Filter testcases\"\n",
      "PP-160\n",
      "Link Entity Filter testcases\n",
      "Creating issue \"Assign categories to Time Filter testcases\"\n",
      "PP-160\n",
      "Link Time Filter testcases\n",
      "Creating issue \"Assign categories to Benchmark Panel testcases\"\n",
      "PP-160\n",
      "Link Benchmark Panel testcases\n",
      "Creating issue \"Assign categories to Benchmark Average on Graph testcases\"\n",
      "PP-160\n",
      "Link Benchmark Average on Graph testcases\n",
      "Creating issue \"Assign categories to Benchmark Percenile on Graph testcases\"\n",
      "PP-160\n",
      "Link Benchmark Percenile on Graph testcases\n",
      "Creating issue \"Assign categories to Benchmark Average & Percentile on Map testcases\"\n",
      "PP-160\n",
      "Link Benchmark Average & Percentile on Map testcases\n",
      "Creating issue \"Assign categories to Labels testcases\"\n",
      "PP-160\n",
      "Link Labels testcases\n",
      "Creating issue \"Assign categories to New Graph implementation (Priyanka) testcases\"\n",
      "PP-160\n",
      "Link New Graph implementation (Priyanka) testcases\n",
      "Creating issue \"Assign categories to Smoke Test testcases\"\n",
      "PP-160\n",
      "Link Smoke Test testcases\n"
     ]
    }
   ],
   "source": [
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import Font\n",
    "\n",
    "workbook = load_workbook(filename='summary.xlsx')\n",
    "sheet = workbook['Sheet1']\n",
    "\n",
    "i = 1\n",
    "while sheet['A' + str(i)].value is not None:\n",
    "\n",
    "    print(sheet['A' + str(i)].value)\n",
    "    print(sheet['c'+str(i)].value + sheet['d'+str(i)].value + sheet['e'+str(i)].value)\n",
    "    issue = jira.issue_create({\n",
    "            'summary': 'Assign categories to ' + sheet['d'+str(i)].value + ' testcases',\n",
    "            # 'description': sheet[schema['columns']['Description'] + str(index)].value,\n",
    "            'project': {'key': 'PP'},\n",
    "            'issuetype': {'name': 'Sub-task'},\n",
    "            'parent': {'key': 'PP-162'},\n",
    "            # 'Priority': 'priority',\n",
    "            # 'Status': 'status',\n",
    "            # 'Creator': 'creator',\n",
    "            # 'Assignee': 'assignee',\n",
    "        })\n",
    "\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "February 22, 2021, 5:13 PM\n"
     ]
    }
   ],
   "source": [
    "print('February 22, 2021, 5:13 PM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " 1.1 , 1.12.1 - 1.12.3 , 1 - 3, 1.3.5 - 1.3.9 \n[' 1.1 ', '1.12.1', '1.12.2', '1.12.3', '1', '2', '3', '1.3.5', '1.3.6', '1.3.7', '1.3.8', '1.3.9']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "txt = ' 1.1 , 1.12.1 - 1.12.3 , 1 - 3, 1.3.5 - 1.3.9 '\n",
    "\n",
    "print(txt)\n",
    "print(str(parse_dependencies(txt)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "get_wbs_lowest_level_task_number('3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print([3,53].append([3,65]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-9a5739bc42a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "range(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}